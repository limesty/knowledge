---
tags:
  - 学习/心理学
status: 尚未完成
---
# 回归初步

## 本章概览

两个变量能够建立完美的线性关系的情况在心理学的研究情境中是极其罕见的。只能通过统计方法，建立与数据最佳拟合的直线，达到描述和预测的目的。在本章中，我们将利用最小平方法求得最佳拟合直线，并建立起回归方程。

## 学习要点

1. 理解什么是回归
2. 学会建立回归方程的计算方法
3. 学习考查回归方程准确性的方法
4. 了解如何解释回归方程
5. 了解一元线性回归的统计前提

## 回归方程

对一个线性方程 (linear equations)，有 $Y=bX+a$，我们称 $b$ 为**斜率** (slope)，$a$ 为**截距** (intercept)。通常，在心理学研究情境中的两个变量无法建立完美的线性关系，只能通过统计方法，建立与数据最佳拟合的直线，达到描述和预测的目的。回归就是用来寻找数据**最佳拟合线**的统计技术，最后建立的直线就是回归线，而与直线对应的方程，就是**回归方程**。

## 回归表达形式

对于**简单线性回归** (Simple Linear Regression) ，有 $Y=b+aX+\epsilon$。其中 $\epsilon$ 为**残差** (Residual/error)。也可使用矩阵表达。

## 回归方程的计算

考察每个点, 比较 $Y$ 的观测值与 $Y$ 的预测值 $\hat{Y}$，$SS_{\text{误差}}=\sum(Y-\hat{Y})^2$。

与实际数据点的距离平方和最小的直线就是**最佳拟合线**，这种确定最佳拟合线的方法就被称为**最小平方法** (least-squares solution) 。

此处不详细给出**最小二乘法** (ordinary least squares, OLS) 的推导过程，仅给出推导结果如下所示：

$$\text{斜率}=b=\dfrac{SP}{SS_X}$$
$$\text{截距}=a=\bar{Y}-b\bar{X}$$

其中，$SP$ 为离差的乘积和，即

$$SP=\sum XY-\dfrac{\sum X\sum Y}{n}$$

$SS_X$ 为 $X$ 的误差平方和，即

$$SS_X=\sum X^2-\dfrac{(\sum X)^2}{n}$$

对于斜率 $b$，还有一个常用的替换公式 $b=r\left(\dfrac{S_Y}{S_X}\right)$，其中 $S_Y$ 和 $S_X$ 分别是 $Y$ 和 $X$ 的标准差，$r=\dfrac{SP}{\sqrt{SS_XSS_Y}}$ 为相关系数。

> 回归方程不能对 $X$ 值范围之外的数据作出预测。

## 回归估计的标准误

 - 回归方程允许我们作出预测，但未给出预测准确性的信息。
 - 估计的标准误给出了回归线与数据点之间标准距离的量度。
 - 回归估计的标准误在概念上类似标准差。

### 计算回归估计的标准误

1. 首先计算误差和方 $SS_{error}=\sum (Y-\hat{Y})^2$
2. 将误差的和方除以自由度 $df=n-2$ 得到误差的方差或称误差的均方。
3. 将误差的方差取平方根得到标准误：

$$S_{误差}=\sqrt{\dfrac{\sum (Y-\hat{Y})^2}{n-2}}=\sqrt{\dfrac{SS_{error}}{df}}=\sqrt{MS_{error}}$$

### 标准误和相关系数之间的关系

相关系数与误差的和方之间的关系可以用公式 $SS_{error}=(1-r^2)SS_Y$ 表示。将其带入计算回归估计的标准误的公式中可以得到：

$$S_{\text{误差}}=\sqrt{\dfrac{SS_{error}}{df}}=\sqrt{\dfrac{(1-r^2)SS_Y}{df}}$$

## 一元线性回归的假设检验

### F 检验

回归模型建立后就要考察自变量是否能够显著预测因变量。如果自变量 $X$ 不能够显著预测因变量 $Y$ ，那表明 $Y$ 的变化完全是由于随机因素导致的；如果自变量 $X$ 能够显著预测因变量，那么自变量 $X$ 对因变量 $Y$ 的解释方差应该显著大于随机因素对因变量 $Y$ 的解释方差。因为二者都服从卡方分布，所以用 $F$ 检验，即 $F=\dfrac{MS_{reg}}{MS_{error}}$。因此，回归分析的方差检验如下表所示：

|     | $df$  | $SS$                            | $MS$               | $F$                   |
| --- | ----- | ------------------------------- | ------------------ | --------------------- |
| 回归  | 1     | $\sum (\hat{Y}-\overline{Y})^2$ | $SS_{reg}$         | $MS_{reg}/MS_{error}$ |
| 残差  | $N-2$ | $\sum (Y-\hat{Y})^2$            | $SS_{error}/(N-2)$ |                       |
| 总和  | $N-1$ | $\sum (Y-\overline{Y})^2$       |                    |                       |

### 路径系数的假设检验

回归分析的前提假设是正态分布，所以截距项服从均值为 $\mu_a$，方差为 $\sigma_a^2$ 的正态分布 $N(\mu_a,\sigma_a^2)$，类似的，自变量的路径系数也服从正态分布 $N(\mu_b,\sigma_b^2)$。因此，二者的随机抽样样本服从 $t$ 分布。

路径系数的假设检验包括截距项的假设检验和自变量的路径系数的假设检验。二者的虚无假设都是 $\mu=0$，这意味着二者都是和 $0$ 进行比较，如果显著的不等于 $0$，表明存在显著的预测效应；否则不存在显著的预测效应。

截距项 $a$ 的方差为 $S_a^2=MS_{error}\left[\dfrac{1}{n}+\dfrac{\overline{X}}{\sum (X-\overline{X})^2}\right]=\dfrac{MS_{error}\sum X^2}{n\sum X^2-(\sum X)^2}$，假设检验 $t$ 统计量为 $t=\dfrac{a-0}{\sqrt{S_a^2}}=\dfrac{a-0}{SE}$，$df=n-2$。

路径系数 $b$ 的方差为 $S_b^2=\dfrac{MS_{error}}{\sum (X-\overline{X})^2}=\dfrac{MS_{error}}{SS_X}$，假设检验 $t$ 统计量为 $t=\dfrac{b-0}{\sqrt{S_b^2}}=\dfrac{b-0}{SE}$，$df=n-2$。

## 一元线性回归的效应量



## 一元线性回归的统计前提